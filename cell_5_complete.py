#######################
## KOM√ìRKA 5 ##
#######################

# KOM√ìRKA 5: DEFINICJE - Funkcje Przetwarzania Audio (WhisperX)
# Ta kom√≥rka izoluje DEFINICJE funkcji zwiƒÖzanych z przetwarzaniem AI.
# Uruchomienie tej kom√≥rki nie wywo≈Çuje ≈ºadnej z tych funkcji,
# a jedynie udostƒôpnia je w pamiƒôci notatnika.
# 
# ZALE≈ªNO≈öCI: Wymaga uruchomienia kom√≥rki 2 (zmienne globalne: DEVICE, COMPUTE_TYPE, etc.)

print("üéôÔ∏è Definiujƒô funkcje przetwarzania audio WhisperX...")
print("=" * 60)

# =============================================================================
# INICJALIZACJA MODELI WHISPERX
# =============================================================================

def initialize_models(hf_token):
    """
    Inicjalizuje wszystkie modele potrzebne do transkrypcji i diaryzacji.
    
    Args:
        hf_token (str): Token HuggingFace do modeli diaryzacji
    
    Returns:
        dict: S≈Çownik z za≈Çadowanymi modelami lub None w przypadku b≈Çƒôdu
    """
    try:
        import whisperx
        from google.colab import userdata
        
        print("üöÄ Inicjalizacja modeli WhisperX...")
        print("=" * 40)
        
        models = {}
        
        # =================================================================
        # 1. ≈ÅADOWANIE MODELU WHISPER
        # =================================================================
        print(f"üì° ≈Åadowanie modelu Whisper: {WHISPER_MODEL}")
        print(f"üñ•Ô∏è UrzƒÖdzenie: {DEVICE}")
        print(f"üî¢ Typ oblicze≈Ñ: {COMPUTE_TYPE}")
        
        try:
            whisper_model = whisperx.load_model(
                WHISPER_MODEL, 
                device=DEVICE, 
                compute_type=COMPUTE_TYPE
                # Brak language parameter - auto-detekcja jƒôzyka
                # Obs≈Çuguje polski, angielski i inne jƒôzyki automatycznie
            )
            models['whisper'] = whisper_model
            print("‚úÖ Model Whisper za≈Çadowany pomy≈õlnie")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd ≈Çadowania modelu Whisper: {e}")
            return None
        
        # =================================================================
        # 2. ≈ÅADOWANIE MODELU ALIGNMENT
        # =================================================================
        print("üéØ ≈Åadowanie modelu alignment...")
        
        try:
            # Automatyczny wyb√≥r modelu alignment dla jƒôzyka polskiego
            alignment_model, metadata = whisperx.load_align_model(
                language_code="pl", 
                device=DEVICE
            )
            models['alignment'] = alignment_model
            models['alignment_metadata'] = metadata
            print("‚úÖ Model alignment za≈Çadowany pomy≈õlnie")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd ≈Çadowania modelu alignment: {e}")
            print("‚ö†Ô∏è Kontynuacja bez alignmentu - timestampy mogƒÖ byƒá mniej precyzyjne")
            models['alignment'] = None
            models['alignment_metadata'] = None
        
        # =================================================================
        # 3. ≈ÅADOWANIE MODELU DIARYZACJI (POPRAWIONE)
        # =================================================================
        print("üë• ≈Åadowanie modelu diaryzacji...")
        
        # Wy≈ÇƒÖczenie TF32 dla stabilno≈õci PyAnnote (zgodnie z GitHub Issue #1370)
        print("üîß Wy≈ÇƒÖczanie TensorFloat-32 dla stabilno≈õci diaryzacji...")
        import torch
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False
        print("‚úÖ TF32 wy≈ÇƒÖczone - zwiƒôkszona stabilno≈õƒá PyAnnote")

        try:
            if not hf_token:
                print("‚ùå Brak HF_TOKEN - diaryzacja niedostƒôpna")
                models['diarization'] = None
            else:
                # Za≈Çadowanie modelu speaker diarization zgodnie z referencyjnym skryptem
                diarization_model = whisperx.diarize.DiarizationPipeline(
                    use_auth_token=hf_token, 
                    device=DEVICE
                )
                models['diarization'] = diarization_model
                print("‚úÖ Model diaryzacji za≈Çadowany pomy≈õlnie")
                
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd ≈Çadowania modelu diaryzacji: {e}")
            print("üí° Sprawd≈∫ czy:")
            print("   - HF_TOKEN jest poprawny")
            print("   - Zaakceptowa≈Çe≈õ warunki modeli pyannote na HuggingFace")
            models['diarization'] = None
        
        # =================================================================
        # PODSUMOWANIE ZA≈ÅADOWANYCH MODELI
        # =================================================================
        print()
        print("üìä PODSUMOWANIE ZA≈ÅADOWANYCH MODELI:")
        print(f"   üé§ Whisper: {'‚úÖ' if models.get('whisper') else '‚ùå'}")
        print(f"   üéØ Alignment: {'‚úÖ' if models.get('alignment') else '‚ùå'}")
        print(f"   üë• Diarization: {'‚úÖ' if models.get('diarization') else '‚ùå'}")
        
        # Sprawdzenie dostƒôpnej pamiƒôci GPU
        if DEVICE == "cuda":
            try:
                import torch
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                print(f"   üíæ GPU Memory: {allocated:.1f} / {gpu_memory:.1f} GB u≈ºywane")
            except:
                pass
        
        print("‚úÖ Inicjalizacja modeli zako≈Ñczona")
        return models
        
    except Exception as e:
        print(f"‚ùå Krytyczny b≈ÇƒÖd inicjalizacji modeli: {e}")
        return None


# =============================================================================
# PRZETWARZANIE PLIK√ìW AUDIO
# =============================================================================

def process_audio_file(audio_path, models):
    """
    Przetwarza pojedynczy plik audio przez WhisperX z pe≈Çnym pipeline.
    
    Args:
        audio_path (str): ≈öcie≈ºka do pliku audio
        models (dict): S≈Çownik z za≈Çadowanymi modelami
    
    Returns:
        dict: Dane transkrypcji z diaryzacjƒÖ lub None w przypadku b≈Çƒôdu
    """
    try:
        import whisperx
        from pathlib import Path
        
        filename = Path(audio_path).name
        print(f"üéµ Przetwarzanie: {filename}")
        print("=" * 50)
        
        # Sprawdzenie czy plik istnieje
        if not Path(audio_path).exists():
            print(f"‚ùå Plik nie istnieje: {audio_path}")
            return None
        
        # Sprawdzenie czy modele sƒÖ za≈Çadowane
        if not models or not models.get('whisper'):
            print("‚ùå Model Whisper nie jest za≈Çadowany")
            return None
        
        # =================================================================
        # ETAP 1: TRANSKRYPCJA WHISPER
        # =================================================================
        print("üé§ Etap 1/3: Transkrypcja Whisper...")
        
        try:
            # Za≈Çadowanie pliku audio
            audio = whisperx.load_audio(audio_path)
            
            # Transkrypcja - zgodnie z referencyjnym skryptem
            result = models['whisper'].transcribe(
                audio, 
                batch_size=BATCH_SIZE  # U≈ºywa globalnej konfiguracji z kom√≥rki 2
            )
            
            print(f"‚úÖ Transkrypcja zako≈Ñczona: {len(result.get('segments', []))} segment√≥w")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd transkrypcji: {e}")
            return None
        
        # =================================================================
        # ETAP 2: WORD-LEVEL ALIGNMENT
        # =================================================================
        print("üéØ Etap 2/3: Word-level alignment...")
        
        try:
            if models.get('alignment') and models.get('alignment_metadata'):
                # Precyzyjne wyr√≥wnanie timestamp√≥w
                result = whisperx.align(
                    result['segments'], 
                    models['alignment'], 
                    models['alignment_metadata'], 
                    audio, 
                    DEVICE, 
                    return_char_alignments=False
                )
                print("‚úÖ Alignment zako≈Ñczony - precyzyjne timestampy")
            else:
                print("‚ö†Ô∏è Alignment pominiƒôty - u≈ºywane surowe timestampy")
                
        except Exception as e:
            print(f"‚ö†Ô∏è B≈ÇƒÖd alignment: {e}")
            print("   Kontynuacja z surowymi timestampami...")
        
        # =================================================================
        # ETAP 3: SPEAKER DIARIZATION (POPRAWIONE)
        # =================================================================
        print("üë• Etap 3/3: Speaker diarization...")
        
        try:
            if models.get('diarization'):
                # Diaryzacja zgodnie z referencyjnym skryptem WhisperX
                print("üîÑ Uruchamianie diaryzacji...")
                
                # Wywo≈Çanie modelu diaryzacji - z ograniczeniem do 2 m√≥wc√≥w
                diarize_segments = models['diarization'](audio, min_speakers=1, max_speakers=3)
                
                # Przypisanie speaker√≥w do segment√≥w - metoda WhisperX
                result = whisperx.assign_word_speakers(diarize_segments, result)
                
                # Zliczenie unikatowych speaker√≥w
                speakers = set()
                for segment in result.get('segments', []):
                    if 'speaker' in segment:
                        speakers.add(segment['speaker'])
                
                print(f"‚úÖ Diaryzacja zako≈Ñczona: {len(speakers)} m√≥wiƒÖcych")
                print(f"   Identyfikowani m√≥wcy: {', '.join(sorted(speakers))}")
                
            else:
                print("‚ö†Ô∏è Diaryzacja pominiƒôta - brak modelu")
                # Dodanie domy≈õlnego speakera
                for segment in result.get('segments', []):
                    segment['speaker'] = 'SPEAKER_00'
                
        except Exception as e:
            print(f"‚ö†Ô∏è B≈ÇƒÖd diaryzacji: {e}")
            print("   Przypisywanie domy≈õlnego speakera...")
            # Fallback - jeden speaker
            for segment in result.get('segments', []):
                segment['speaker'] = 'SPEAKER_00'
        
        # =================================================================
        # FINALIZACJA I STATYSTYKI
        # =================================================================
        
        # Obliczenie statystyk
        total_segments = len(result.get('segments', []))
        total_duration = 0
        word_count = 0
        
        for segment in result.get('segments', []):
            if 'end' in segment and 'start' in segment:
                total_duration += segment['end'] - segment['start']
            if 'text' in segment:
                word_count += len(segment['text'].split())
        
        print()
        print("üìä STATYSTYKI PRZETWARZANIA:")
        print(f"   ‚è±Ô∏è Ca≈Çkowity czas: {total_duration:.1f}s")
        print(f"   üìù Segment√≥w: {total_segments}")
        print(f"   üî§ S≈Ç√≥w: {word_count}")
        print(f"   üìÑ ≈örednio s≈Ç√≥w/segment: {word_count/max(total_segments,1):.1f}")
        
        print(f"‚úÖ Przetwarzanie {filename} zako≈Ñczone pomy≈õlnie")
        return result
        
    except Exception as e:
        print(f"‚ùå Nieoczekiwany b≈ÇƒÖd przetwarzania {audio_path}: {e}")
        return None


# =============================================================================
# FUNKCJE POMOCNICZE AUDIO (POPRAWIONE)
# =============================================================================

def validate_audio_file(audio_path):
    """
    Sprawdza czy plik audio jest prawid≈Çowy i mo≈ºe byƒá przetworzony.
    Obs≈Çuguje niestandardowe parametry MP3 (16kHz, 32kbps).
    """
    try:
        from pathlib import Path
        import torchaudio
        
        path = Path(audio_path)
        
        # Sprawdzenie istnienia pliku
        if not path.exists():
            print(f"‚ùå Plik nie istnieje: {audio_path}")
            return False
        
        # Sprawdzenie rozszerzenia
        valid_extensions = {'.mp3', '.wav', '.m4a', '.flac', '.ogg'}
        if path.suffix.lower() not in valid_extensions:
            print(f"‚ö†Ô∏è Nieobs≈Çugiwane rozszerzenie: {path.suffix}")
            return False
        
        # Sprawdzenie rozmiaru pliku
        file_size = path.stat().st_size
        if file_size == 0:
            print(f"‚ùå Plik jest pusty: {audio_path}")
            return False
        
        # Pr√≥ba za≈Çadowania pliku audio - pierwsze podej≈õcie przez WhisperX
        try:
            import whisperx
            # WhisperX loader jest bardziej tolerancyjny dla niestandardowych MP3
            audio_data = whisperx.load_audio(audio_path)
            
            # Sprawdzenie minimalnej d≈Çugo≈õci (16kHz * 0.1s = 1600 sampli)
            if len(audio_data) < 1600:
                print(f"‚ùå Plik zbyt kr√≥tki: {len(audio_data)} sampli")
                return False
            
            duration = len(audio_data) / 16000  # WhisperX normalizuje do 16kHz
            print(f"‚úÖ Plik audio prawid≈Çowy: {duration:.1f}s, 16kHz (WhisperX)")
            return True
            
        except Exception as whisperx_error:
            print(f"‚ö†Ô∏è WhisperX loader b≈ÇƒÖd: {whisperx_error}")
            
            # Fallback: torchaudio
            try:
                waveform, sample_rate = torchaudio.load(audio_path)
                duration = waveform.shape[1] / sample_rate
                
                if duration < 0.1:
                    print(f"‚ùå Plik zbyt kr√≥tki: {duration:.2f}s")
                    return False
                
                print(f"‚úÖ Plik audio prawid≈Çowy: {duration:.1f}s, {sample_rate}Hz (TorchAudio)")
                return True
                
            except Exception as torchaudio_error:
                print(f"‚ùå TorchAudio b≈ÇƒÖd: {torchaudio_error}")
                print(f"üí° Sprawd≈∫ format pliku: {audio_path}")
                return False
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd walidacji pliku {audio_path}: {e}")
        return False


def cleanup_audio_processing():
    """
    Czyszczenie pamiƒôci po przetwarzaniu audio.
    """
    try:
        import torch
        import gc
        
        # Garbage collection
        gc.collect()
        
        # Czyszczenie cache GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("üßπ Wyczyszczono pamiƒôƒá GPU")
        
        print("üßπ Cleanup zako≈Ñczony")
        
    except Exception as e:
        print(f"‚ö†Ô∏è B≈ÇƒÖd cleanup: {e}")


print()
print("=" * 60)
print("‚úÖ KOM√ìRKA 5 - DEFINICJE FUNKCJI WHISPERX ZAKO≈ÉCZONA")
print("=" * 60)
print("üìã Zdefiniowane funkcje:")
print("   üöÄ initialize_models() - inicjalizacja WhisperX + diaryzacja")
print("   üéµ process_audio_file() - pe≈Çny pipeline transkrypcji")
print("   ‚úÖ validate_audio_file() - walidacja plik√≥w audio")
print("   üßπ cleanup_audio_processing() - czyszczenie pamiƒôci")
print()
print("üéØ Nastƒôpny krok: G≈Ç√≥wna logika aplikacji (orkiestrator)")
print("‚ñ∂Ô∏è Uruchom kom√≥rkƒô 6, aby zdefiniowaƒá funkcjƒô main_process()") 